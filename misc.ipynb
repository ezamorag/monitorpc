{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM4(nn.Module):\n",
    "    def __init__(self, z_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, z_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(z_dim, a_dim)\n",
    "\n",
    "    def forward(self, zt):\n",
    "        yt, _ = self.lstm(zt)\n",
    "        at = self.fc_out(yt) # output shape: (N, L, output_size)\n",
    "        return at\n",
    "    \n",
    "class LSTM5(nn.Module):  #Numero de neuronas ocultas diferente a z_dim\n",
    "    def __init__(self, z_dim, h_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(h_dim, a_dim)\n",
    "\n",
    "    def forward(self, zt):\n",
    "        yt, _ = self.lstm(zt)\n",
    "        at = self.fc_out(yt) # output shape: (N, L, output_size)\n",
    "        return at\n",
    "    \n",
    "class LSTM6(nn.Module):  # añadir tanh a la salida\n",
    "    def __init__(self, z_dim, h_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(h_dim, a_dim)\n",
    "\n",
    "    def forward(self, zt):\n",
    "        yt, _ = self.lstm(zt)\n",
    "        at = F.tanh(self.fc_out(yt)) # output shape: (N, L, output_size)\n",
    "        return at\n",
    "    \n",
    "class LSTM7(nn.Module):  # 2 capas de LSTM\n",
    "    def __init__(self, z_dim, h_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, num_layers=2, batch_first=True)\n",
    "        self.fc_out = nn.Linear(h_dim, a_dim)\n",
    "\n",
    "    def forward(self, zt):\n",
    "        yt, _ = self.lstm(zt)\n",
    "        at = F.tanh(self.fc_out(yt)) # output shape: (N, L, output_size)\n",
    "        return at\n",
    "    \n",
    "class LSTM8wtanh(nn.Module):  \n",
    "    def __init__(self, z_dim, h_dim, num_layers, a_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(h_dim, a_dim)\n",
    "\n",
    "    def forward(self, zt):\n",
    "        yt, _ = self.lstm(zt)\n",
    "        at = F.tanh(self.fc_out(yt)) # output shape: (N, L, output_size)\n",
    "        return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entendiendo la diferencia entre output y h_n en LSTM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mmm = nn.LSTM( input_size = 1, hidden_size = 10, num_layers  = 1 )\n",
    "output, (hn, cn) = mmm(torch.rand( 50, 1, 1))\n",
    "\n",
    "print(output.shape, hn.shape)\n",
    "\n",
    "print(output[-1,:,:])\n",
    "print()\n",
    "print(hn)\n",
    "\n",
    "# Checa aca el dibujoo\n",
    "# https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 0.1 de la sincronizacion \n",
    "from monitorlib import load_pcdata\n",
    "import pandas as pd\n",
    "\n",
    "def sync_positions(data,sample,i):\n",
    "     # Searching for the cursor position that it is nearest to screenshot timestamp ti \n",
    "     # (before or after, it does not matter)\n",
    "     ti = data[sample]['screenshots']['timestamp'][i]\n",
    "     abs_diff = abs(data[sample]['moves']['timestamp'] - ti)\n",
    "     closest_index = abs_diff.idxmin()\n",
    "     px, py = data[sample]['moves']['px'][closest_index], data[sample]['moves']['py'][closest_index]\n",
    "     return px, py\n",
    "\n",
    "def sync_MKevents(sample_df, data, sample):\n",
    "     \n",
    "     def set_value(sample_df, abs_diff, event):\n",
    "          closest_index = abs_diff.idxmin()\n",
    "          if sample_df['mouse_keyboard'][closest_index] != None:\n",
    "               print('colision')\n",
    "               # what to do?\n",
    "               sample_df.loc[closest_index, 'mouse_keyboard'] = event\n",
    "          else:\n",
    "               sample_df.loc[closest_index, 'mouse_keyboard'] = event\n",
    "          \n",
    "\n",
    "     # Initial values\n",
    "     sample_df['mouse_keyboard'] = [None for i in range(len(sample_df))]\n",
    "     timestamps_df = data[sample]['screenshots']['timestamp']\n",
    "\n",
    "     # Clicks\n",
    "     for i in range(len(data[sample]['clicks'])):\n",
    "          ti = data[sample]['clicks']['timestamp'][i]\n",
    "          button = data[sample]['clicks']['button'][i]\n",
    "          set_value(sample_df, abs(timestamps_df - ti), button)\n",
    "\n",
    "     # Scrolls\n",
    "     for i in range(len(data[sample]['scrolls'])):\n",
    "          ti = data[sample]['scrolls']['timestamp'][i]\n",
    "          dy = data[sample]['scrolls']['dy'][i] \n",
    "          if dy == 1:\n",
    "               scroll = 'scroll_down' \n",
    "          elif dy == -1:\n",
    "               scroll = 'scroll_up'\n",
    "          else:\n",
    "               scroll = 'what?_scroll'\n",
    "          set_value(sample_df, abs(timestamps_df - ti), scroll)\n",
    "   \n",
    "     # Keyboard events\n",
    "     for i in range(len(data[sample]['keys'])):\n",
    "          ti = data[sample]['keys']['timestamp'][i]\n",
    "          key = data[sample]['keys']['key'][i].strip(\"'\")\n",
    "          set_value(sample_df, abs(timestamps_df - ti), key)\n",
    "\n",
    "def sync(data):\n",
    "     samples = []\n",
    "     for sample in data.keys():\n",
    "          sample_df = pd.DataFrame()\n",
    "          sample_df['img_path'] = [img_path for img_path in data[sample]['screenshots']['img_path']]\n",
    "          \n",
    "          # Sync cursor positions to screenshots\n",
    "          positions = [sync_positions(data,sample,i) for i in range(len(data[sample]['screenshots']))]\n",
    "          sample_df['px'] = [px for px, _ in positions]\n",
    "          sample_df['py'] = [py for _, py in positions]\n",
    "\n",
    "          # Sync mouse and keyboard events to screenshots\n",
    "          sync_MKevents(sample_df, data, sample)\n",
    "\n",
    "          samples.append(sample_df)\n",
    "     return samples\n",
    "\n",
    "data = load_pcdata('data') \n",
    "\n",
    "# Syncronization\n",
    "samples = sync(data)\n",
    "\n",
    "# Replace None values with a string\n",
    "for sample in samples:\n",
    "    sample.fillna(\"no_action\",inplace=True)\n",
    "\n",
    "# tokenization\n",
    "tokens = set()\n",
    "for sample in samples:\n",
    "    tokens.update(sample['mouse_keyboard'].unique().tolist())\n",
    "tokens = list(tokens)\n",
    "tokens.sort()\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "\n",
    "# Add timestamp column\n",
    "for sample in samples:\n",
    "    sample['time'] = sample['img_path'].map(lambda x: float(x.split('/')[2].split('_')[-1].split('.jpg')[0]))\n",
    "\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Version 0.0 de la sincronizacion \n",
    "from monitorlib import load_pcdata\n",
    "import pandas as pd\n",
    "\n",
    "# Tengo mal este codigo en la sincronizacion: porque hay mas clicks y teclas apretadas que as que deberia haber. \n",
    "# Debo buscar en lcicks y pressed_keys y asociarlas a los tiempos de las iamgenes.  No barrer sobre las imagenes. \n",
    "# lo mismo para los scrolls, quizas para las poscines dle curso no es necesairos porque son mas abundantes que las iamgenes\n",
    "\n",
    "data = load_pcdata('data')\n",
    "\n",
    "SyncTHclicks = 0.2\n",
    "SyncTHscrolls = 0.3\n",
    "SyncTHkeys = 0.3 \n",
    "\n",
    "def sync(data,sample,i):\n",
    "     # Cursor positions\n",
    "     ti = data[sample]['screenshots']['timestamp'][i]\n",
    "     abs_diff = abs(data[sample]['moves']['timestamp'] - ti)\n",
    "     closest_index = abs_diff.idxmin()\n",
    "     mx, my = data[sample]['moves']['px'][closest_index], data[sample]['moves']['py'][closest_index]\n",
    "\n",
    "     # Clicks\n",
    "     abs_diff = abs(data[sample]['clicks']['timestamp'] - ti)\n",
    "     if abs_diff.min() < SyncTHclicks: \n",
    "          closest_index = abs_diff.idxmin()\n",
    "          cx, cy = data[sample]['clicks']['px'][closest_index], data[sample]['clicks']['py'][closest_index]  \n",
    "          button = data[sample]['clicks']['button'][closest_index]\n",
    "     else:\n",
    "          cx, cy, button = None, None, 'no action'\n",
    "\n",
    "     # Scrolls\n",
    "     abs_diff = abs(data[sample]['scrolls']['timestamp'] - ti)\n",
    "     if abs_diff.min() < SyncTHscrolls: \n",
    "          closest_index = abs_diff.idxmin()\n",
    "          dx, dy = data[sample]['scrolls']['dx'][closest_index], data[sample]['scrolls']['dy'][closest_index] \n",
    "          if dy == 1:\n",
    "               scroll = 'scroll_down' \n",
    "          elif dy == -1:\n",
    "               scroll = 'scroll_up'\n",
    "          else:\n",
    "               scroll = 'what?_scroll'\n",
    "     else:\n",
    "          scroll = 'no action'\n",
    "          \n",
    "\n",
    "     # Keyboard events\n",
    "     abs_diff = abs(data[sample]['keys']['timestamp'] - ti)\n",
    "     if abs_diff.min() < SyncTHkeys: \n",
    "          closest_index = abs_diff.idxmin()\n",
    "          key = data[sample]['keys']['key'][closest_index].strip(\"'\")\n",
    "     else:\n",
    "          key = 'no action'\n",
    "\n",
    "     return mx, my, cx, cy, button, scroll, key\n",
    "\n",
    "img_paths, peripherals = [], []\n",
    "for sample in data.keys():\n",
    "     img_paths.extend([img_path for img_path in data[sample]['screenshots']['img_path']])\n",
    "     peripherals.extend(sync(data,sample,i) for i in range(len(data[sample]['screenshots'])))\n",
    "\n",
    "pc_df = pd.DataFrame(img_paths, columns=['img_path'])\n",
    "knames = ['mouse_px', 'mouse_py', 'click_px', 'click_py', 'click_button', 'scroll', 'pressed_key']\n",
    "for i,kname in enumerate(knames):\n",
    "     pc_df[kname] = [x[i] for x in peripherals]\n",
    "\n",
    "pc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retardos puestos y debugging de las coordenadas manualmente\n",
    "# para samples[0]\n",
    "actions = [('Button.left', (28, 131), 0.01), \n",
    "           ('Button.left', (1754, 45),0.01),\n",
    "           ('z', (1754, 45), 0.01),\n",
    "           ('o', (1754, 46), 0.01),\n",
    "           ('o', (1754, 46), 0.01), \n",
    "           ('Key.enter', (1754, 46), 6),\n",
    "           ('Button.left', (1040, 600), 2),  #(_,567)\n",
    "           ('Button.left', (1075, 275), 1),\n",
    "           ('Button.left', (1017, 564), 1),\n",
    "           ('Button.left', (1080, 294), 1),\n",
    "           ('Button.left', (1100, 765), 5), #(1056, 725)\n",
    "           ('Button.left', (946, 755), 1),\n",
    "           ('Button.left', (943, 769), 1),\n",
    "           ('Button.left', (1025, 790), 1),  #(998, 775)\n",
    "           ('Button.left', (929, 828), 1),\n",
    "           ('Button.left', (427, 1057), 1),\n",
    "           ('Button.left', (665, 746), 0.01),\n",
    "]\n",
    "\n",
    "# Tool for debugging actions\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "img = Image.open('notable1.png')\n",
    "draw = ImageDraw.Draw(img)\n",
    "points = []\n",
    "for i, (_,point, _) in enumerate(actions, start=1):\n",
    "    draw.text(point, str(i), fill='red')\n",
    "    points.append(point)\n",
    "draw.polygon(points, outline='red')\n",
    "#img.show()\n",
    "\n",
    "time.sleep(2)\n",
    "for action in actions:\n",
    "    pcontroller(action[0], action[1])\n",
    "    time.sleep(action[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probando Depth Anything\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "x = np.asarray(Image.open('../../Downloads/rgb.jpg')).copy()\n",
    "d = np.asarray(Image.open('../../Downloads/depth.png'))\n",
    "\n",
    "mask = 255*np.ones((1080,1920,1))\n",
    "mask[d > 50] = 0\n",
    "n = np.concatenate([x,mask],axis=2)\n",
    "\n",
    "\n",
    "img = Image.fromarray(n.astype('uint8'))\n",
    "img.show()\n",
    "img.save('../../Downloads/crop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dEnfocando la mirada en lo importante e ignorando el resto\n",
    "\n",
    "### pensandolo mejor conviene meter dos imagenes una zoom out y otra zoom in cuya posicion este controlada por el agente. \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def focuscrop(x1, y1, dw, factor):\n",
    "    def reduce(img, box, factor):\n",
    "        im = img.crop(box)\n",
    "        return im.resize((im.size[0]//factor,im.size[1]//factor))\n",
    "    \n",
    "    im = img.crop((x1,y1,x1+dw,y1+dw))\n",
    "    im1 = reduce(img, (0,0,x1,h), factor)\n",
    "    im2 = reduce(img, (x1+dw,0,w,h), factor)\n",
    "    im3 = reduce(img, (x1,0,x1+dw,y1), factor)\n",
    "    im4 = reduce(img, (x1,y1+dw,x1+dw,h), factor)\n",
    "    return [im, im1, im2, im3, im4]\n",
    "\n",
    "img = Image.open(data['sample1']['screenshots']['img_path'][4])\n",
    "w, h = img.size\n",
    "x = np.asarray(img).copy()\n",
    "\n",
    "imgs = focuscrop(x1=1920//2, y1 = 1080//2, dw = 300, factor = 5)\n",
    "[imgs[i].show() for i in range(len(imgs))]\n",
    "\n",
    "npixels_1 = [imgs[i].size[0]*imgs[i].size[1]*3 for i in range(len(imgs))]\n",
    "npixels_1 = np.asarray(npixels_1).sum()\n",
    "npixels_o = img.size[0]*img.size[1]*3\n",
    "print(npixels_o)\n",
    "print(npixels_1, round(npixels_1/npixels_o,4)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usan la api de open ai para interpretar el screenhoot\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "api_key =\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_path = data['sample1']['screenshots']['img_path'][6]\n",
    "base64_image = encode_image(image_path)\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "payload = { \n",
    "  \"model\": \"gpt-4-vision-preview\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What’s in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
