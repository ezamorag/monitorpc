{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "Z_DIM = 32\n",
    "VOC_ACTIONS = ['no_action', 'Button.left', 'Key.enter', 'Key.esc', 'o', 'scroll_down', 'z']\n",
    "MAX_LENGTH = 25 #steps\n",
    "MAXWIDTH_screenshoot = 1920  #pixels\n",
    "MAXHEIGHT_screenshoot = 1080 #pixels\n",
    "MAXDELAY = 15 #secs\n",
    "SEED = 0\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezamorag/miniconda3/envs/automat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=valid)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=valid)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=valid)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=valid)\n",
       "  (fc_mean): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc_logvar): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,4, stride=2, padding=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32,64,4, stride=2, padding=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64,128,4, stride=2, padding=\"valid\")\n",
    "        self.conv4 = nn.Conv2d(128,256,4, stride=2, padding=\"valid\")\n",
    "        self.fc_mean = nn.Linear(256*2*2, latent_dims)\n",
    "        self.fc_logvar = nn.Linear(256*2*2, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.normal.Normal(torch.tensor(0.0).to(device), \n",
    "                                                   torch.tensor(1.0).to(device))\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        z_mu = self.fc_mean(x)\n",
    "        z_logvar = self.fc_logvar(x)\n",
    "        z_sigma = torch.exp(0.5*z_logvar)\n",
    "        \n",
    "        z = z_mu + z_sigma*self.N.sample(z_mu.shape)\n",
    "        self.kl = 0.5*(z_sigma**2 + z_mu**2 - z_logvar - 1).sum()\n",
    "        return z, z_mu, z_sigma\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1,(1024,1,1)),\n",
    "            nn.ConvTranspose2d(1024,128,5, stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,5, stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,32,6, stride=2, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32,3,6, stride=2, padding=0),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Vae(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(device)\n",
    "        z, _, _ = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "from torchvision import transforms\n",
    "\n",
    "IMGSIZE= [64,64] #[1080//10, 1920//10]\n",
    "img_transform = transforms.Compose([\n",
    "                transforms.Resize(IMGSIZE, antialias=None),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                    std=[0.5, 0.5, 0.5]),\n",
    "                \n",
    "                ])\n",
    "\n",
    "vae = Vae(latent_dims=Z_DIM)\n",
    "vae.to(device)\n",
    "vae.load_state_dict(torch.load('best_vae.pt')['model_state_dict'])\n",
    "vae.eval()\n",
    "vae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "  \n",
    "class LSTM13(nn.Module):  \n",
    "    def __init__(self, z_dim, h_dim, hfc_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1_action = nn.Linear(h_dim, hfc_dim)\n",
    "        self.fc2_action = nn.Linear(hfc_dim, 1)\n",
    "        self.fc_position = nn.Linear(h_dim, 2)\n",
    "        self.fc_delay = nn.Linear(h_dim, 1)\n",
    "\n",
    "    def forward(self, zt, state):\n",
    "        ht, state = self.lstm(zt, state)  # (N, L, h_dim)\n",
    "        at = self.fc2_action(F.relu(self.fc1_action(ht)))\n",
    "        pt = self.fc_position(ht)\n",
    "        dt = F.relu(self.fc_delay(ht))\n",
    "        return at, pt, dt, state\n",
    "    \n",
    "lstm_best = LSTM13(z_dim=32, h_dim=256, hfc_dim=1024, num_layers=1)\n",
    "lstm_best.to(device)\n",
    "lstm_best.load_state_dict(torch.load('best_lstm13_vae.pt')['model_state_dict'])\n",
    "\n",
    "# ¿Pongo el estado internamente o exteernamente? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM14(nn.Module):  \n",
    "    def __init__(self, z_dim, h_dim, hfc_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.lstm = nn.LSTM(z_dim, h_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1_action = nn.Linear(z_dim + h_dim, hfc_dim)\n",
    "        self.fc2_action = nn.Linear(hfc_dim, 1)\n",
    "        self.fc_position = nn.Linear(z_dim + h_dim, 2)\n",
    "        self.fc_delay = nn.Linear(z_dim + h_dim, 1)\n",
    "\n",
    "    def forward(self, zt, state):  # both zt,ht as inputs\n",
    "        ht, state = self.lstm(zt, state)  # (N, L, h_dim)\n",
    "        xt = torch.cat((zt,ht), -1)\n",
    "        at = self.fc2_action(F.relu(self.fc1_action(xt)))\n",
    "        pt = self.fc_position(xt)\n",
    "        dt = F.relu(self.fc_delay(xt))\n",
    "        return at, pt, dt, state\n",
    "\n",
    "lstm_best = LSTM14(z_dim=32, h_dim=256, hfc_dim=1024, num_layers=1)\n",
    "lstm_best.to(device)\n",
    "lstm_best.load_state_dict(torch.load('best_lstm14.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1.0256]]), tensor([[ 0.0686, -0.4251]]), tensor([[0.]]))\n",
      "[torch.Size([1, 1]), torch.Size([1, 2]), torch.Size([1, 1])]\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1,32))\n",
    "state = (torch.zeros((1,lstm_best.h_dim)), torch.zeros((1,lstm_best.h_dim)))\n",
    "with torch.no_grad():\n",
    "    out = lstm_best(x, state)\n",
    "    state = out[-1]\n",
    "\n",
    "print(out[:3])\n",
    "print([out[i].shape for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "\n",
    "def extract_features(image): \n",
    "    img_tensor = img_transform(image)\n",
    "    with torch.no_grad():\n",
    "        _, zmu, _ = vae.encoder(torch.unsqueeze(img_tensor, 0))\n",
    "    return zmu\n",
    "\n",
    "def pcontroller(action, position, delay): \n",
    "    # Mouse\n",
    "    if \"Button.\" in action or \"scroll_\" in action:\n",
    "        if action == 'Button.left': \n",
    "            pyautogui.moveTo(position[0],position[1])\n",
    "            pyautogui.click()\n",
    "        elif action == 'scroll_down': \n",
    "            pyautogui.moveTo(position[0],position[1])\n",
    "            pyautogui.scroll(-1)\n",
    "    # Keyboard\n",
    "    elif \"Key.\" in action: \n",
    "        if action == 'Key.enter':\n",
    "            pyautogui.moveTo(position[0],position[1])\n",
    "            pyautogui.press('enter')\n",
    "        elif action == 'Key.esc':\n",
    "            pyautogui.moveTo(position[0],position[1])\n",
    "            pyautogui.press('esc')\n",
    "    # Write with keyboard\n",
    "    elif len(action) == 1 and action.isalpha():\n",
    "        pyautogui.moveTo(position[0],position[1])\n",
    "        pyautogui.write(action)\n",
    "    else:\n",
    "        print(f'This action {action} does not exist')\n",
    "    time.sleep(delay)\n",
    "\n",
    "def execute(at, pt, dt, debugging=False):\n",
    "    at = at.squeeze().detach().cpu().numpy()\n",
    "    pt = pt.squeeze().detach().cpu().numpy()\n",
    "    dt = dt.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    def unnormalized(x_norm, max):\n",
    "        x = max/2*(x_norm + 1.0)\n",
    "        return int(round(x,0))\n",
    "    \n",
    "    action = VOC_ACTIONS[unnormalized(at,len(VOC_ACTIONS))]\n",
    "    px = unnormalized(pt[0],MAXWIDTH_screenshoot)\n",
    "    py = unnormalized(pt[1],MAXHEIGHT_screenshoot)\n",
    "    delay = MAXDELAY*dt\n",
    "    if debugging == False:\n",
    "        pcontroller(action, (px, py), delay)\n",
    "    return action, (px,py), delay\n",
    "\n",
    "\n",
    "# Tool for debugging actions\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def viz_actions(img, index, action, px, py, delay):\n",
    "    mfont = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "    kfont = ImageFont.truetype(\"arial.ttf\", 80)\n",
    "    size = 10\n",
    "    \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    if 'Button.' in action: \n",
    "        draw.ellipse([px-size/2,py-size/2,px+size//2,py+size//2], fill=\"red\")\n",
    "        draw.point((px,py), fill='yellow')\n",
    "        draw.text((px,py), str(index), fill='red', font=mfont)\n",
    "    else:\n",
    "        draw.text((MAXWIDTH_screenshoot//2 - 100,MAXHEIGHT_screenshoot//2), action, fill='red', font=kfont)\n",
    "    img.save('runs/step{:02}_delay{}.jpg'.format(index, str(round(delay, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "actions_df = pd.read_csv('actions_df - sample0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1\n",
      "0.5712 0.0152 -0.8511 0.0\n",
      "scroll_down (975, 80) 0.0\n",
      "\n",
      "step = 2\n",
      "0.7516 0.437 -0.9946 0.0\n",
      "z (1380, 3) 0.0\n",
      "\n",
      "step = 3\n",
      "-0.1172 -0.2628 -0.86 0.0\n",
      "Key.esc (708, 76) 0.0\n",
      "\n",
      "step = 4\n",
      "-0.3984 -0.3146 -0.6146 0.016\n",
      "Key.enter (658, 208) 0.233\n",
      "\n",
      "step = 5\n",
      "-0.4669 0.1174 -0.5325 0.312\n",
      "Key.enter (1073, 252) 4.673\n",
      "\n",
      "step = 6\n",
      "-0.6115 -0.3915 -0.5158 0.007\n",
      "Button.left (584, 261) 0.101\n",
      "\n",
      "step = 7\n",
      "-0.2095 -0.3037 -0.1684 0.277\n",
      "Key.esc (668, 449) 4.162\n",
      "\n",
      "step = 8\n",
      "-0.4461 -0.0376 -0.3124 0.087\n",
      "Key.enter (924, 371) 1.31\n",
      "\n",
      "step = 9\n",
      "-0.4313 0.0026 -0.134 0.176\n",
      "Key.enter (963, 468) 2.647\n",
      "\n",
      "step = 10\n",
      "-0.5023 0.0979 -0.2115 0.12\n",
      "Key.enter (1054, 426) 1.793\n",
      "\n",
      "step = 11\n",
      "-0.4962 0.0975 -0.1286 0.153\n",
      "Key.enter (1054, 471) 2.3\n",
      "\n",
      "step = 12\n",
      "-0.5167 0.1244 -0.1546 0.137\n",
      "Key.enter (1079, 457) 2.048\n",
      "\n",
      "step = 13\n",
      "-0.5154 0.1253 -0.1389 0.14\n",
      "Key.enter (1080, 465) 2.106\n",
      "\n",
      "step = 14\n",
      "-0.517 0.1305 -0.1405 0.135\n",
      "Key.enter (1085, 464) 2.029\n",
      "\n",
      "step = 15\n",
      "-0.516 0.1313 -0.1372 0.134\n",
      "Key.enter (1086, 466) 2.006\n",
      "\n",
      "step = 16\n",
      "-0.5159 0.1326 -0.1376 0.131\n",
      "Key.enter (1087, 466) 1.964\n",
      "\n",
      "step = 17\n",
      "-0.5154 0.1326 -0.137 0.129\n",
      "Key.enter (1087, 466) 1.941\n",
      "\n",
      "step = 18\n",
      "-0.5151 0.1327 -0.1372 0.128\n",
      "Key.enter (1087, 466) 1.918\n",
      "\n",
      "step = 19\n",
      "-0.5147 0.1325 -0.1372 0.127\n",
      "Key.enter (1087, 466) 1.904\n",
      "\n",
      "step = 20\n",
      "-0.5144 0.1323 -0.1374 0.126\n",
      "Key.enter (1087, 466) 1.891\n",
      "\n",
      "step = 21\n",
      "-0.5141 0.1321 -0.1376 0.125\n",
      "Key.enter (1087, 466) 1.882\n",
      "\n",
      "step = 22\n",
      "-0.5139 0.1319 -0.1377 0.125\n",
      "Key.enter (1087, 466) 1.875\n",
      "\n",
      "step = 23\n",
      "-0.5137 0.1318 -0.1379 0.125\n",
      "Key.enter (1086, 466) 1.87\n",
      "\n",
      "step = 24\n",
      "-0.5135 0.1316 -0.138 0.124\n",
      "Key.enter (1086, 465) 1.865\n",
      "\n",
      "step = 25\n",
      "-0.5134 0.1315 -0.1381 0.124\n",
      "Key.enter (1086, 465) 1.862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time.sleep(3)\n",
    "state = (torch.zeros((1,lstm_best.h_dim)), torch.zeros((1,lstm_best.h_dim)))\n",
    "for i in range(MAX_LENGTH):  #\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    #screenshot = Image.open(actions_df['img_path'][i])\n",
    "    zt = extract_features(screenshot)\n",
    "    with torch.no_grad():\n",
    "        at, pt, dt, state = lstm_best(zt, state)\n",
    "    action, (px,py), delay = execute(at,pt, dt, debugging=False)\n",
    "    if action == \"no_action\":\n",
    "        pyautogui.alert('The execution has been terminated.')\n",
    "        break\n",
    "\n",
    "    print(f'step = {i+1}')\n",
    "    print(round(float(at[0,0]),4), round(float(pt[0,0]),4), round(float(pt[0,1]),4), round(float(dt[0,0]),3))\n",
    "    print(action, (px,py), round(delay, 3))\n",
    "    print()\n",
    "\n",
    "    viz_actions(screenshot, i, action, px, py, delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEB27 2024\n",
    "\n",
    "Me parece que si deb limitar el rango de at y pt usando tanh [-1,1] porque a veces puede generar valores fuera del rango. \n",
    "\n",
    "Recuerda que las Zoom cambio un poco su webapp\n",
    "\n",
    "\n",
    "Use los screenshots guardados del sample0 y funciono bien todo el resto del codigo. Esto me idica que el problema esta en el Autoencoder que \n",
    "genera features sensibles a los cambios en el screenshot. \n",
    "\n",
    "Hey! podria equitar nuevaos samples con screenshot diferentes aprovechando que lstm si predice bien. \n",
    "\n",
    "Probe entrenando LSTM13 con VAE, hace que las decisiones de la posicion varien un poco del ideal, asi que no le atina a los botones. Y varian cada vez que se ejecuta. \n",
    "\n",
    "Probe el modelo LSTM14 que contatena zty ht, me parece que es un modelo qe podria generalizar mas que el LSTM13 y LSTM12. \n",
    "\n",
    "Conclusion: mi codigo es completo y funcional, pero el modelo no generaliza lo suficiente. \n",
    "\n",
    "\n",
    "¿Que hago ahora?\n",
    "1) colecto, reentreno modelos, y lo puebo en condiciones iguales -> grabo el demo y compacto el pipeline \n",
    "2) mejoro la recoleccion de datos, recolecto una base de datos mas grande, y entreno un modelo screen -> (M&K action, pt, dt) sin tareas, solo imtar la dinamica\n",
    "   Con el fin de ajustarlo con pocos jemplos despues para la tarea de abrir la sesion de zoom \n",
    "3) Solo recolecto mas ejemplos de la misma tarea, y busco que generalize haciendo esa tarea. \n",
    "\n",
    "4) añado las acciones pasadas como entrada al LSTM14 \n",
    "5) Aplico la filosofica de JEPA para entrenar un modelo fundacional. \n",
    "\n",
    "6) Replicar el trabjao de deepmind 2022\n",
    "\n",
    "\n",
    "RW para implentar modelos del mundo \n",
    "Predicting the Future with Simple World Models https://www.scholar-inbox.com/papers/Saanum2024ARXIV_Predicting_the_Future_with.pdf\n",
    "WORLD MODEL ON MILLION-LENGTH VIDEO AND LANGUAGE WITH RINGATTENTION  https://www.scholar-inbox.com/papers/Liu2024ARXIV_World_Model_on_Million.pdf\n",
    "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment  https://www.scholar-inbox.com/papers/Tang2024ARXIV_WorldCoder_a_Model_Based.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
